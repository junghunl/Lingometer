{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1SCZ6_1tnbau",
   "metadata": {
    "executionInfo": {
     "elapsed": 13956,
     "status": "ok",
     "timestamp": 1758050788805,
     "user": {
      "displayName": "이중훈",
      "userId": "00116443405035277094"
     },
     "user_tz": -540
    },
    "id": "1SCZ6_1tnbau"
   },
   "outputs": [],
   "source": [
    "from os.path import join as pjoin\n",
    "from pprint import pformat\n",
    "import yaml\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "import copy, math\n",
    "\n",
    "from personal_VAD.utils import *\n",
    "from personal_VAD.dataloader_specOnline import get_dataloader as get_online_dataloader\n",
    "from personal_VAD.dataloader import get_dataloader\n",
    "from personal_VAD.models.ASPVAD import TotalLoss\n",
    "\n",
    "import torch\n",
    "import torch.amp as amp\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bEmFqoX3nYIp",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1758050788817,
     "user": {
      "displayName": "이중훈",
      "userId": "00116443405035277094"
     },
     "user_tz": -540
    },
    "id": "bEmFqoX3nYIp"
   },
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def __init__(self, mode='train', thres=0.5, eps=1e-8):\n",
    "        self.eps = eps\n",
    "        self.mode = mode\n",
    "        self.thres = thres\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.tp, self.fp, self.tn, self.fn = 0, 0, 0, 0\n",
    "        self.emb_correct = 0\n",
    "        self.total_spk = 0\n",
    "        self.loss_sum = 0\n",
    "        self.loss_count = 0\n",
    "        if self.mode!='train':\n",
    "            self.logits = []\n",
    "            self.labels = []\n",
    "\n",
    "    def update(self, loss, logits, labels, spk_logits, spk_labels):\n",
    "        preds = (logits > self.thres).astype(int)\n",
    "        self.tp += np.sum((preds == 1) & (labels == 1))\n",
    "        self.fp += np.sum((preds == 1) & (labels == 0))\n",
    "        self.tn += np.sum((preds == 0) & (labels == 0))\n",
    "        self.fn += np.sum((preds == 0) & (labels == 1))\n",
    "\n",
    "        pred_spk = np.argmax(spk_logits, axis=-1)\n",
    "        self.emb_correct += np.sum(pred_spk == spk_labels)\n",
    "        self.total_spk += len(spk_labels)\n",
    "\n",
    "        self.loss_sum += loss\n",
    "        self.loss_count += 1\n",
    "\n",
    "        if self.mode!='train':\n",
    "            self.logits.append(logits)\n",
    "            self.labels.append(labels)\n",
    "\n",
    "    def compute(self):\n",
    "        if self.total_spk==0: return defaultdict(float)\n",
    "\n",
    "        avg_loss = self.loss_sum / self.loss_count\n",
    "        acc = (self.tp + self.tn) / (self.tp + self.fp + self.tn + self.fn)\n",
    "        spk_acc = self.emb_correct / self.total_spk\n",
    "        if self.mode=='train': return {'loss': avg_loss, 'acc': acc, 'spk_acc': spk_acc}\n",
    "\n",
    "        precision = self.tp / (self.tp + self.fp + self.eps)\n",
    "        recall = self.tp / (self.tp + self.fn + self.eps)\n",
    "        f1 = 2 * precision * recall / (precision + recall + self.eps)\n",
    "        logits = np.concatenate(self.logits)\n",
    "        labels = np.concatenate(self.labels)\n",
    "        ap = average_precision_score(labels, logits)\n",
    "        auc = roc_auc_score(labels, logits)\n",
    "        return {'loss': avg_loss, 'acc': acc, 'spk_acc': spk_acc, 'f1': f1, 'ap': ap, 'roc': auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D_1yd0Y8ogwn",
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1758050788856,
     "user": {
      "displayName": "이중훈",
      "userId": "00116443405035277094"
     },
     "user_tz": -540
    },
    "id": "D_1yd0Y8ogwn"
   },
   "outputs": [],
   "source": [
    "def train(**kwargs):\n",
    "    with open(kwargs['config']) as con_read: yaml_config = yaml.load(con_read, Loader=yaml.FullLoader)\n",
    "    args = dict(yaml_config, **kwargs)\n",
    "    exp_dir = validate_path(args['exp_dir'], is_dir=True)\n",
    "    model_dir = validate_path(pjoin(exp_dir,\"models\"), is_dir=True)\n",
    "    with open(pjoin(exp_dir, 'config.yaml'), 'w') as fout: fout.write(yaml.dump(args))\n",
    "\n",
    "    logger = get_logger(exp_dir, 'train.log')\n",
    "    set_seed(args.get('seed', 0))\n",
    "    device = get_device()\n",
    "    logger.info(f\"device: {device}\")\n",
    "    logger.info(\"<== Passed Arguments ==>\")\n",
    "    for line in pformat(args).split('\\n'): logger.info(line)\n",
    "\n",
    "    args['model_args']['num_speakers'] = args[\"dataset_args\"][\"speaker_num\"]\n",
    "    model, start_epoch = load_model(args['model'], args['model_args'], device=device, get_epoch=True, do_compile=args.get('do_compile', False))\n",
    "    logger.info(\"<== Model ==>\")\n",
    "    logger.info(f'model size: {sum(param.numel() for param in model.parameters())}')\n",
    "    #for line in pformat(model).split('\\n'): logger.info(line)\n",
    "    if start_epoch>1: logger.info(f'checkpoint loaded. start_epoch: {start_epoch}')\n",
    "\n",
    "    dataloader_args = args['dataloader_args']\n",
    "    dataset_args = args['dataset_args']\n",
    "    train_dataloader = get_online_dataloader(dataset_args.get(\"train_dataset_args\",None), dataloader_args)\n",
    "    val_dataloader = get_dataloader(dataset_args.get(\"val_dataset_args\",None), dataloader_args, eval=True)\n",
    "    test_dataloader = get_dataloader(dataset_args.get(\"test_dataset_args\",None), dataloader_args, eval=True)\n",
    "    logger.info(\"<== Dataloaders ==>\")\n",
    "    #logger.info(f\"train data num: {len(train_dataloader.dataset)}\")\n",
    "    logger.info(f\"val data num: {len(val_dataloader.dataset)}\")\n",
    "    logger.info(f\"test data num: {len(test_dataloader.dataset)}\")\n",
    "\n",
    "    criterion = TotalLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), **args['optimizer_args'])\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, **args['scheduler_args'])\n",
    "    scaler = amp.GradScaler(device.type, enabled=(args.get('enable_amp',False) and device.type=='cuda'))\n",
    "\n",
    "    logger.info(\"<========== Training process ==========>\")\n",
    "    logger.info(\"+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\")\n",
    "    logger.info(f\"|{'Epoch':>10}|{'Lr':>10}|{'Train_Loss':>10}|{'Train_Acc':>10}|{'Spk_Acc':>10}|{'Val_Loss':>10}|{'Val_Acc':>10}|{'Val_AP':>10}|{'Val_AUC':>10}|{'Val_F1':>10}|\")\n",
    "    logger.info(\"+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\")\n",
    "    best_loss, best_loss_epoch, best_loss_model_state_dict = float('inf'), 0, None\n",
    "    best_acc, best_acc_epoch, best_acc_model_state_dict = 0, 0, None\n",
    "    best_f1, best_f1_epoch, best_f1_model_state_dict = 0, 0, None\n",
    "    model.train()\n",
    "    val_interval = args['val_interval']\n",
    "    first_val = math.ceil(start_epoch / val_interval) * val_interval\n",
    "    for i, step in enumerate(range(first_val, args['max_steps']+1, val_interval)):\n",
    "        if i==0: train_len = first_val-start_epoch+1\n",
    "        else: train_len = val_interval\n",
    "        train_metrics = train_steps(model, train_dataloader, criterion, optimizer, scaler, train_len)\n",
    "        val_metrics = evaluate(model, val_dataloader, criterion)\n",
    "        train_loss = train_metrics['loss']\n",
    "        val_loss = val_metrics['loss']\n",
    "        val_acc = val_metrics['acc']\n",
    "        val_f1 = val_metrics['f1']\n",
    "        #scheduler.step()\n",
    "\n",
    "        logger.info(f\"|{step:>10}|{scheduler.get_last_lr()[0]:>10.4f}|{train_loss:>10.6f}|{train_metrics['acc']:>10.4f}|{train_metrics['spk_acc']:>10.4f}|{val_loss:>10.6f}|{val_metrics['acc']:>10.4f}|{val_metrics['ap']:>10.4f}|{val_metrics['roc']:>10.4f}|{val_metrics['f1']:>10.4f}|\")\n",
    "\n",
    "        if step % args['save_interval'] == 0:\n",
    "            save_checkpoint(model, pjoin(model_dir, f'model_{step}.pt'))\n",
    "        if best_loss > val_loss: best_loss, best_loss_epoch, best_loss_model_state_dict = val_loss, step, copy.deepcopy(model.state_dict())\n",
    "        if best_acc < val_acc: best_acc, best_acc_epoch, best_acc_model_state_dict = val_acc, step, copy.deepcopy(model.state_dict())\n",
    "        if best_f1 < val_f1: best_f1, best_f1_epoch, best_f1_model_state_dict = val_f1, step, copy.deepcopy(model.state_dict())\n",
    "\n",
    "    logger.info(\"+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\")\n",
    "\n",
    "    if best_loss_model_state_dict:\n",
    "        save_checkpoint(best_loss_model_state_dict, pjoin(model_dir, f'best_loss_model_{best_loss_epoch}.pt'))\n",
    "    if best_f1_model_state_dict:\n",
    "        save_checkpoint(best_f1_model_state_dict, pjoin(model_dir, f'best_f1_model_{best_f1_epoch}.pt'))\n",
    "    if best_acc_model_state_dict:\n",
    "        save_checkpoint(best_acc_model_state_dict, pjoin(model_dir, f'best_acc_model_{best_acc_epoch}.pt'))\n",
    "        model.load_state_dict(best_acc_model_state_dict)\n",
    "\n",
    "    val_metrics = evaluate(model, val_dataloader, criterion)\n",
    "    logger.info(f\"<========== Best validation value : epoch {best_acc_epoch} ==========>\")\n",
    "    logger.info(\"+----------+----------+----------+----------+----------+\")\n",
    "    logger.info(f\"|{'val_Loss':>10}|{'val_Acc':>10}|{'val_AP':>10}|{'val_AUC':>10}|{'val_F1':>10}|\")\n",
    "    logger.info(\"+----------+----------+----------+----------+----------+\")\n",
    "    logger.info(f\"|{val_metrics['loss']:>10.6f}|{val_metrics['acc']:>10.4f}|{val_metrics['ap']:>10.4f}|{val_metrics['roc']:>10.4f}|{val_metrics['f1']:>10.4f}|\")\n",
    "    logger.info(\"+----------+----------+----------+----------+----------+\")\n",
    "\n",
    "    test_metrics = evaluate(model, test_dataloader, criterion)\n",
    "    logger.info(\"<========== Test process ==========>\")\n",
    "    logger.info(\"+----------+----------+----------+----------+----------+\")\n",
    "    logger.info(f\"|{'test_Loss':>10}|{'test_Acc':>10}|{'test_AP':>10}|{'test_AUC':>10}|{'test_F1':>10}|\")\n",
    "    logger.info(\"+----------+----------+----------+----------+----------+\")\n",
    "    logger.info(f\"|{test_metrics['loss']:>10.6f}|{test_metrics['acc']:>10.4f}|{test_metrics['ap']:>10.4f}|{test_metrics['roc']:>10.4f}|{test_metrics['f1']:>10.4f}|\")\n",
    "    logger.info(\"+----------+----------+----------+----------+----------+\")\n",
    "\n",
    "\n",
    "def train_steps(model: torch.nn.Module, dataloader, criterion, optimizer, scaler, max_steps):\n",
    "    device = next(model.parameters()).device\n",
    "    model.train()\n",
    "    enable_amp = scaler._enabled\n",
    "\n",
    "    metrics = Metrics(mode='train')\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i == max_steps:\n",
    "            break\n",
    "\n",
    "        simul = batch['simul'].to(device, non_blocking=True)    # (B,T,F)\n",
    "        enroll = batch['enroll'].to(device, non_blocking=True)\n",
    "        label = batch['label'].float().to(device, non_blocking=True)  # (B,T)\n",
    "        enroll_length = batch['enroll_len'].to(device, non_blocking=True)\n",
    "        simul_length = batch['simul_len'].to(device, non_blocking=True)\n",
    "        spk = batch['spk'].to(device, non_blocking=True)\n",
    "\n",
    "        with amp.autocast(device.type, enabled=enable_amp):\n",
    "            logit, tilde_s, spk_logit = model(enroll, simul, enroll_length=enroll_length, simul_length=simul_length, spk_label=spk)\n",
    "            loss, _, _, _ = criterion(logit, tilde_s, spk_logit, label, spk)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        mask = torch.arange(label.size(1), device=device)[None, :] < simul_length[:, None]  # (B,T)\n",
    "        metrics.update(loss.item(),\n",
    "                       logit[mask].cpu().detach().numpy(),\n",
    "                       label[mask].cpu().detach().numpy(),\n",
    "                       spk_logit.cpu().detach().numpy(),\n",
    "                       spk.cpu().detach().numpy())\n",
    "\n",
    "    #gc.collect()\n",
    "    return metrics.compute()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model:torch.nn.Module, dataloader, criterion):\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    metrics = Metrics(mode='test')\n",
    "    for batch in dataloader:\n",
    "        simul = batch['simul'].to(device, non_blocking=True)    # (B,T,F)\n",
    "        enroll = batch['enroll'].to(device, non_blocking=True)\n",
    "        label = batch['label'].float().to(device, non_blocking=True)  # (B,T)\n",
    "        enroll_length = batch['enroll_len'].to(device, non_blocking=True)\n",
    "        simul_length = batch['simul_len'].to(device, non_blocking=True)\n",
    "        spk = batch['spk'].to(device, non_blocking=True)\n",
    "\n",
    "        logit, tilde_s, spk_logit = model(enroll, simul, enroll_length=enroll_length, simul_length=simul_length, spk_label=spk)\n",
    "        _, loss, _, _ = criterion(logit, tilde_s, spk_logit, label, spk)\n",
    "\n",
    "        mask = torch.arange(label.size(1), device=device)[None, :] < simul_length[:, None]  # (B,T)\n",
    "        metrics.update(loss.item(),\n",
    "                       logit[mask].cpu().detach().numpy(),\n",
    "                       label[mask].cpu().detach().numpy(),\n",
    "                       spk_logit.cpu().detach().numpy(),\n",
    "                       spk.cpu().detach().numpy())\n",
    "    return metrics.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vKmmorVJqC2w",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "vKmmorVJqC2w"
   },
   "outputs": [],
   "source": [
    "train(config=r'conf/libri.yaml')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "lingometer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
