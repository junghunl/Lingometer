{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f47ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import Dict, List, Tuple, Union, Optional\n",
    "import os, sys, warnings, copy, random\n",
    "from collections import \n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from functools import lru_cache\n",
    "from itertools import combinations\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from scipy.signal import medfilt\n",
    "\n",
    "\n",
    "# ignore torchaudio warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*torchaudio.load_with_torchcodec.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*StreamingMediaDecoder.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*deprecated.*\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from personal_VAD.utils import load_model as load_pvad_model\n",
    "from word_count_estimator.utils import load_model as load_wce_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0483b2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title utils\n",
    "SEED = 42\n",
    "DEFAULT_SR = 16000\n",
    "\n",
    "def second2sample(second, sr=DEFAULT_SR): return int(second*sr)\n",
    "def sample2second(sample, sr=DEFAULT_SR): return int(sample/sr)\n",
    "s2sam = second2sample\n",
    "sam2s = sample2second\n",
    "\n",
    "def mono_resample(wav: torch.Tensor, sr: int=16000, target_sr: int = DEFAULT_SR) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    [C,T] or [T] -> [1,T]\n",
    "    \"\"\"\n",
    "    if wav.dim() == 1: wav = wav.unsqueeze(0)\n",
    "    elif wav.dim() == 2 and wav.size(0) > 1: wav = wav.mean(dim=0, keepdim=True)\n",
    "    if sr != target_sr: wav = torchaudio.functional.resample(wav, sr, target_sr)\n",
    "    return wav\n",
    "\n",
    "@lru_cache(maxsize=40)\n",
    "def load_wav(paths, s=None, e=None, dur=None):\n",
    "    if isinstance(paths, str): paths = [paths]\n",
    "    wavs = [mono_resample(*torchaudio.load(p)) for p in paths]\n",
    "    wav = torch.cat(wavs, dim=1)\n",
    "    if s is not None: wav = wav[:,s2sam(s):s2sam(e)]\n",
    "    if dur is not None:\n",
    "        offset = random.randint(0, wav.size(1)-s2sam(dur))\n",
    "        wav = wav[:,offset:offset+s2sam(dur)]\n",
    "    return wav\n",
    "\n",
    "class Extractor:\n",
    "    def __init__(self, apply_cmvn=True, no_batch_time_first=False, **feature_args):\n",
    "        if feature_args==None: feature_args={}\n",
    "        basic_args = {'sample_rate':16000, 'n_fft':400, 'n_mels':24, 'win_length':400, 'hop_length':160}\n",
    "        basic_args.update(feature_args)\n",
    "        self.extractor = torchaudio.transforms.MelSpectrogram(**basic_args)\n",
    "        self.apply_cmvn = apply_cmvn\n",
    "        self.no_batch_time_first = no_batch_time_first\n",
    "\n",
    "    def __call__(self, wav):\n",
    "        return self.extract(wav)\n",
    "\n",
    "    def extract(self, wav:list|str):\n",
    "        if isinstance(wav,str): wav, sr = torchaudio.load(wav)\n",
    "        spec = self.extractor(wav)  # (1,F,T)\n",
    "        spec = torch.log10(spec + 1e-6)\n",
    "        if self.apply_cmvn:\n",
    "            mean = spec.mean(dim=2, keepdim=True)\n",
    "            std = spec.std(dim=2, keepdim=True)\n",
    "            spec = (spec - mean) / (std + 1e-9)\n",
    "        if self.no_batch_time_first:\n",
    "            spec = spec.squeeze(0).transpose(0,1)  # (T,F)\n",
    "        return spec\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Evaluation Metrics & Helper Utils\n",
    "# -----------------------------------------------------------------------------\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    tp = int(np.sum((y_true == 1) & (y_pred == 1)))\n",
    "    fp = int(np.sum((y_true == 0) & (y_pred == 1)))\n",
    "    fn = int(np.sum((y_true == 1) & (y_pred == 0)))\n",
    "    tn = int(np.sum((y_true == 0) & (y_pred == 0)))\n",
    "\n",
    "    eps = 1e-12\n",
    "    prec   = tp / (tp + fp + eps)\n",
    "    rec    = tp / (tp + fn + eps)\n",
    "    f1     = 2 * prec * rec / (prec + rec + eps)\n",
    "    iou    = tp / (tp + fp + fn + eps)\n",
    "    miss   = fn / (tp + fn + eps)\n",
    "    fa     = fp / (tn + fp + eps)\n",
    "\n",
    "    metrics = {\n",
    "        \"TPf\": tp, \"FPf\": fp, \"FNf\": fn, \"TNf\": tn,\n",
    "        \"precision\": prec, \"recall\": rec, \"f1\": f1, \"iou\": iou,\n",
    "        \"miss_rate\": miss, \"fa_rate\": fa,\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def choose_enroll(df, min_dur):\n",
    "    df = df.copy()\n",
    "    valid_df = df[df[\"duration\"] >= min_dur]\n",
    "\n",
    "    if not valid_df.empty:\n",
    "        sampled = valid_df.sample(1)\n",
    "        sampled[\"_original_idxs\"] = sampled.index.tolist()\n",
    "        sampled[\"_st_et_list\"] = sampled.apply(lambda row: [(row[\"start_time\"], row[\"end_time\"])], axis=1)\n",
    "        return sampled\n",
    "\n",
    "    df_sorted = df.sort_values(\"duration\", ascending=False)\n",
    "    total_dur = 0\n",
    "    selected_idxs = []\n",
    "    st_et_list = []\n",
    "    for idx, row in df_sorted.iterrows():\n",
    "        selected_idxs.append(idx)\n",
    "        st_et_list.append((row[\"start_time\"], row[\"end_time\"]))\n",
    "        total_dur += row[\"duration\"]\n",
    "        if total_dur >= min_dur: break\n",
    "    combined_row = df.loc[selected_idxs].iloc[0].copy()\n",
    "    combined_row[\"audio_path\"] = tuple(df.loc[selected_idxs][\"audio_path\"].tolist())\n",
    "    combined_row[\"duration\"] = total_dur\n",
    "    combined_row[\"_st_et_list\"] = st_et_list\n",
    "    combined_row[\"_original_idxs\"] = selected_idxs\n",
    "    return pd.DataFrame([combined_row])\n",
    "\n",
    "\n",
    "def segs2label(segs, max_t):\n",
    "    label = np.zeros(max_t, dtype=int)\n",
    "    for s,e in segs: label[s:e]=1\n",
    "    return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf615bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# PVAD (Personal Voice Activity Detection)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def PVAD(key_wav, target_wav, pvad_model,\n",
    "         threshold=0.5,\n",
    "         median_filter_size=5,\n",
    "         ns_min_length=300,\n",
    "         s_min_length=200,\n",
    "         apply_cmvn=False,\n",
    "         debug_mode=False\n",
    "         ):\n",
    "    device = next(pvad_model.parameters()).device\n",
    "\n",
    "    extractor = Extractor(apply_cmvn=apply_cmvn)\n",
    "    key_feat = extractor(key_wav).to(device)\n",
    "    target_feat = extractor(target_wav).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logit, _, _ = pvad_model(key_feat, target_feat)\n",
    "        scores = torch.sigmoid(logit.squeeze(0)).cpu().detach().numpy()\n",
    "\n",
    "    if debug_mode:\n",
    "        target_feat = target_feat.squeeze(0).cpu().detach().numpy()\n",
    "        #print(*scores.tolist())\n",
    "        plt.figure(figsize=(20, 4))\n",
    "        plt.imshow(target_feat, origin='lower', aspect='auto', cmap='magma')\n",
    "        plt.colorbar(label='Amplitude (dB)')\n",
    "        plt.xlabel('Time Frame')\n",
    "        plt.ylabel('Mel Bin')\n",
    "        plt.plot(np.arange(target_feat.shape[1]), scores*24, color='cyan', linewidth=2, label='Score')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title('MelSpectrogram with Score Overlay')\n",
    "        plt.show()\n",
    "\n",
    "    binary_mask = scores>=threshold\n",
    "\n",
    "    # median filtering\n",
    "    binary_mask = binary_mask.astype(int)\n",
    "    smoothed_mask = medfilt(binary_mask, kernel_size=median_filter_size)\n",
    "\n",
    "    # get segments (in ms)\n",
    "    smoothed_mask = np.concatenate(([0], smoothed_mask, [0]))\n",
    "    speech_indices = np.where(np.diff(smoothed_mask) != 0)[0]\n",
    "    segments = []\n",
    "    for i in range(0, len(speech_indices), 2):\n",
    "        start_frame = int(speech_indices[i]*10)\n",
    "        end_frame = int(speech_indices[i+1]*10)\n",
    "        segments.append([start_frame, end_frame])\n",
    "    if debug_mode:\n",
    "      print(\"raw segments:\",segments)\n",
    "\n",
    "    # delete short non-speech intervals.\n",
    "    merged_segments = []\n",
    "    current_segment = segments[0] if segments else None\n",
    "    for next_segment in segments[1:]:\n",
    "        nonspeech_duration = next_segment[0] - current_segment[1]\n",
    "        if nonspeech_duration < ns_min_length:\n",
    "            current_segment[1] = next_segment[1]\n",
    "        else:\n",
    "            merged_segments.append(current_segment)\n",
    "            current_segment = next_segment\n",
    "    if current_segment: merged_segments.append(current_segment)\n",
    "    if debug_mode:\n",
    "      print(\"merged segments\", merged_segments)\n",
    "\n",
    "    # delete short speech segments.\n",
    "    valid_segments = []\n",
    "    for segment in merged_segments:\n",
    "        segment_duration = segment[1] - segment[0]\n",
    "        if segment_duration > s_min_length:\n",
    "            valid_segments.append(segment)\n",
    "\n",
    "    # convert ms to sample idx\n",
    "    ms2sample = lambda x: x*16\n",
    "    segments_sampleidx = []\n",
    "    for s,e in valid_segments: segments_sampleidx.append([ms2sample(s),ms2sample(e)])\n",
    "\n",
    "    return segments_sampleidx\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# WCE (Word Count Estimator) Wrapper\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class WCE:\n",
    "    def __init__(\n",
    "        self,\n",
    "        wce_model,\n",
    "        apply_trim: bool = True,\n",
    "        apply_cmvn: bool = True,\n",
    "    ):\n",
    "\n",
    "        self.apply_cmvn = bool(apply_cmvn)\n",
    "        self.apply_trim = bool(apply_trim)\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self.model = wce_model.to(self.device).eval()\n",
    "        self.extractor = Extractor(apply_cmvn=self.apply_cmvn, no_batch_time_first=True)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _to_mono_1ch_16k(self, wav: torch.Tensor, sr: int=16000, target_sr: int = 16000) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        입력: wav [C,T] 또는 [T], sr (샘플레이트)\n",
    "        출력: [1,T] @ target_sr (16kHz 기본)\n",
    "        \"\"\"\n",
    "        if wav.dim() == 1:\n",
    "            wav = wav.unsqueeze(0)  # [1,T]\n",
    "        elif wav.dim() == 2:\n",
    "            if wav.size(0) > 1:\n",
    "                wav = wav.mean(dim=0, keepdim=True)  # [1,T]\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected wav shape: {tuple(wav.shape)}\")\n",
    "        if sr != target_sr:\n",
    "            wav = torchaudio.functional.resample(wav, sr, target_sr)\n",
    "\n",
    "        return wav\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_pred_count(self, output) -> int:\n",
    "        \"\"\"\n",
    "        syllnet: logits -> sigmoid -> 0.5 threshold count\n",
    "        wce    : dict 출력에서 'count' 사용 (모델 구현에 맞게 조정)\n",
    "        \"\"\"\n",
    "        if self.model_type == \"sylnet\":\n",
    "            # 기대 형태: logits [B, U]\n",
    "            logits = output\n",
    "            pred_cnt = (torch.sigmoid(logits) >= 0.5).float().sum(dim=1)  # [B]\n",
    "            return int(pred_cnt.item())\n",
    "        else:  # wce\n",
    "            # 기대 형태: {\"count\": Tensor([B, 1]) 혹은 [B]}\n",
    "            cnt = output[\"count\"]\n",
    "            if isinstance(cnt, (list, tuple)):\n",
    "                cnt = torch.as_tensor(cnt)\n",
    "            if cnt.dim() > 1:\n",
    "                cnt = cnt.squeeze()\n",
    "            return int(cnt[0].item() if cnt.numel() > 1 else cnt.item())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def word_extract(self, wav: Union[str, Path, torch.Tensor]):\n",
    "        \"\"\"\n",
    "        단일 wav -> Log-Mel(+CMVN) -> 모델 추론 -> count(int)\n",
    "        \"\"\"\n",
    "        if isinstance(wav, (str, Path)):\n",
    "            wav, sr = torchaudio.load(str(wav))  # [C,T]\n",
    "            wav = self._to_mono_1ch_16k(wav, sr)      # [T]\n",
    "            if self.apply_trim:\n",
    "              wav = self.trim_silence(wav, trim_db=20)\n",
    "        elif isinstance(wav, torch.Tensor):\n",
    "            sr = 16000  # Tensor 들어오면 sr을 외부에서 보장한다고 가정\n",
    "            wav = self._to_mono_1ch_16k(wav, sr)      # [T]\n",
    "            if self.apply_trim:\n",
    "              wav = self.trim_silence(wav, trim_db=20)\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported type: {type(wav)}\")\n",
    "\n",
    "        spec = self.extractor(wav)       # [T, n_mels]\n",
    "\n",
    "        x_pad = spec.unsqueeze(0).to(self.device)        # [1, T, n_mels]\n",
    "        lengths = torch.tensor([x_pad.size(1)], device=self.device, dtype=torch.long)\n",
    "\n",
    "        output = self.model(x_pad, lengths)\n",
    "\n",
    "        return self.compute_pred_count(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bbff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution and Evaluation Function\n",
    "def run_evaluation_pipeline(key_audio_path, target_audio_path, \n",
    "                            pvad_model_config, pvad_post_processing,\n",
    "                            wce_model_config, gt_word_count=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # --- [Step 1] Load PVAD Model ---\n",
    "    pvad_model = load_pvad_model(\n",
    "        model_name=pvad_model_config[\"model\"],\n",
    "        model_args=pvad_model_config[\"model_args\"],\n",
    "        requires_grad=False,\n",
    "        device=device\n",
    "    )\n",
    "    pvad_model.eval()\n",
    "\n",
    "    # --- [Step 2] Load WCE Model ---\n",
    "    wce_net = load_wce_model(\n",
    "        model_name=wce_model_config[\"model\"],\n",
    "        model_args=wce_model_config[\"model_args\"],\n",
    "        requires_grad=False,\n",
    "        device=device\n",
    "    )\n",
    "    wce_model = WCE(wce_net, apply_trim=False, apply_cmvn=True)\n",
    "\n",
    "    # --- [Step 3] Load Audio and Enroll Speaker ---\n",
    "    # Use load_wav if defined, otherwise fallback to torchaudio\n",
    "    try:\n",
    "        key_wav = load_wav(key_audio_path).to(device)     # Enrollment voice\n",
    "        target_wav = load_wav(target_audio_path).to(device) # Analysis target audio\n",
    "    except NameError:\n",
    "        key_wav, _ = torchaudio.load(key_audio_path)\n",
    "        key_wav = mono_resample(key_wav).to(device)\n",
    "        target_wav, _ = torchaudio.load(target_audio_path)\n",
    "        target_wav = mono_resample(target_wav).to(device)\n",
    "\n",
    "    # --- [Step 4] Run PVAD (Extract Specific Speaker Segments) ---\n",
    "    # Result: segments_sampleidx = [[start, end], ...]\n",
    "    pred_segments = PVAD(\n",
    "        key_wav, \n",
    "        target_wav, \n",
    "        pvad_model,\n",
    "        **pvad_post_processing.get(\"PVAD_post_args\", {}) \n",
    "    )\n",
    "\n",
    "    # --- [Step 5] Run WCE and Evaluate ---\n",
    "    total_pred_words = 0\n",
    "    \n",
    "    for s, e in pred_segments:\n",
    "        seg_wav = target_wav[:, s:e].squeeze(0) # [T]\n",
    "        if seg_wav.size(0) > 160: # Minimum length check\n",
    "            # Estimate word count using WCE class method\n",
    "            count = wce_model.word_extract(seg_wav)\n",
    "            total_pred_words += count\n",
    "\n",
    "    # Print Results and Evaluate\n",
    "    print(f\"--- Analysis Results ---\")\n",
    "    print(f\"Detected Segments: {len(pred_segments)}\")\n",
    "    print(f\"Estimated Total Word Count: {total_pred_words:.2f}\")\n",
    "    \n",
    "    if gt_word_count is not None:\n",
    "        error = abs(gt_word_count - total_pred_words) / max(1, gt_word_count)\n",
    "        print(f\"Ground Truth Word Count: {gt_word_count}\")\n",
    "        print(f\"Word Count Estimation Error Rate: {error:.4%}\")\n",
    "\n",
    "    return total_pred_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6737bfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvad_model_path = {\n",
    "    'libri': '../personal_VAD/conf/libri.yaml',\n",
    "    'chime': '../personal_VAD/conf/chime.yaml',\n",
    "    'ami': '../personal_VAD/conf/ami.yaml'\n",
    "}\n",
    "\n",
    "pvad_post_processing_config = './conf/pvad_post_processing.yaml'\n",
    "wce_model_paths = '../word_count_estimator/conf/wce_frame_onset.yaml'\n",
    "\n",
    "key_audio_path = ...\n",
    "target_audio_path = ...\n",
    "\n",
    "pvad_model_config = OmegaConf.load(pvad_model_path['ami'])\n",
    "pvad_post_processing = OmegaConf.load(pvad_post_processing_config)\n",
    "wce_model_config = OmegaConf.load(wce_model_paths)\n",
    "\n",
    "total_pred_words = run_evaluation_pipeline(\n",
    "    key_audio_path, \n",
    "    target_audio_path, \n",
    "    pvad_model_config, \n",
    "    pvad_post_processing,\n",
    "    wce_model_config\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
