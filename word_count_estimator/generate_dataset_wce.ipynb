{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350c3e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import gzip\n",
    "import pickle\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from decimal import Decimal\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch_audiomentations import ApplyImpulseResponse, AddBackgroundNoise\n",
    "from textgrid import TextGrid\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Ignore unnecessary warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*torchaudio.load_with_torchcodec.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*StreamingMediaDecoder.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*deprecated.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee73fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augmentor:\n",
    "    \"\"\"Handles Audio Augmentation (RIR, Background Noise).\"\"\"\n",
    "    def __init__(self, rir_paths=None, noise_paths=None, rir_prob=0, noise_prob=0, min_snr_in_db=3, max_snr_in_db=30):\n",
    "        self.do_aug = False if rir_prob==0 and noise_prob==0 else True\n",
    "        if not self.do_aug: return\n",
    "        \n",
    "        self.rir_augmentor = ApplyImpulseResponse(\n",
    "            ir_paths=rir_paths, sample_rate=16000, p=rir_prob, output_type='tensor'\n",
    "        ) if rir_paths else None\n",
    "        \n",
    "        self.noise_augmentor = AddBackgroundNoise(\n",
    "            background_paths=noise_paths, sample_rate=16000, \n",
    "            min_snr_in_db=min_snr_in_db, max_snr_in_db=max_snr_in_db,\n",
    "            p=noise_prob, output_type='tensor'\n",
    "        ) if noise_paths else None\n",
    "\n",
    "    def __call__(self, wav):\n",
    "        return self.augment(wav)\n",
    "\n",
    "    def augment(self, wav):\n",
    "        if not self.do_aug: return wav\n",
    "        if wav.dim() == 1: wav = wav.unsqueeze(0).unsqueeze(0) # (1, 1, T)\n",
    "        elif wav.dim() == 2: wav = wav.unsqueeze(0) # (1, C, T)\n",
    "\n",
    "        if self.rir_augmentor: wav = self.rir_augmentor(wav)\n",
    "        if self.noise_augmentor: wav = self.noise_augmentor(wav)\n",
    "        \n",
    "        return wav.squeeze(0) # Return (C, T)\n",
    "\n",
    "class Extractor:\n",
    "    \"\"\"Handles Feature Extraction (LogMel Spectrogram + CMVN).\"\"\"\n",
    "    def __init__(self, apply_cmvn=True, **feature_args):\n",
    "        self.apply_cmvn = apply_cmvn\n",
    "        basic_args = {'sample_rate':16000, 'n_fft':400, 'n_mels':24, 'win_length':400, 'hop_length':160}\n",
    "        basic_args.update(feature_args)\n",
    "        self.extractor = torchaudio.transforms.MelSpectrogram(**basic_args)\n",
    "\n",
    "    def __call__(self, wav):\n",
    "        return self.extract(wav)\n",
    "\n",
    "    def extract(self, wav: torch.Tensor):\n",
    "        if wav.dim() == 1: wav = wav.unsqueeze(0) # Ensure (1, T) for mono\n",
    "        \n",
    "        spec = self.extractor(wav)  # (1, F, T)\n",
    "        spec = spec.squeeze(0).transpose(0, 1)  # (T, F)\n",
    "        spec = torch.log10(spec + 1e-6)\n",
    "        \n",
    "        if self.apply_cmvn:\n",
    "            mean = spec.mean(dim=0, keepdim=True)\n",
    "            std = spec.std(dim=0, keepdim=True)\n",
    "            spec = (spec - mean) / (std + 1e-9)\n",
    "            \n",
    "        return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc105f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "from decimal import Decimal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from textgrid import TextGrid\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "SR = 16000\n",
    "HOP = 160  # 10ms\n",
    "VOWEL_PATTERN = re.compile(r\"^(AA|AE|AH|AO|AW|AY|EH|ER|EY|IH|IY|OW|OY|UH|UW)\\d$\")\n",
    "\n",
    "def count_words_and_chars(text: str):\n",
    "    cleaned = re.sub(r'^\\W+|\\W+$', '', text)\n",
    "    words = cleaned.split()\n",
    "    num_words = len(words)\n",
    "    num_chars = sum(c.isalpha() for c in cleaned)\n",
    "    return num_words, num_chars\n",
    "\n",
    "def parse_speakers_txt(speakers_txt_path: Path):\n",
    "    speakers_info = {}\n",
    "    if not speakers_txt_path.exists(): return {}\n",
    "    with open(speakers_txt_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\";\"): continue\n",
    "            parts = line.strip().split(\"|\")\n",
    "            if len(parts) >= 3:\n",
    "                speaker_id = parts[0].strip()\n",
    "                gender = parts[1].strip()\n",
    "                subset = parts[2].strip()\n",
    "                speakers_info[speaker_id] = {\"gender\": gender, \"subset\": subset}\n",
    "    return speakers_info\n",
    "\n",
    "def parse_chapters_txt(chapters_txt_path: Path):\n",
    "    chapters_info = {}\n",
    "    if not chapters_txt_path.exists(): return {}\n",
    "    with open(chapters_txt_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\";\") or not line.strip(): continue\n",
    "            parts = line.strip().split(\"|\")\n",
    "            if len(parts) < 7: continue\n",
    "            chapter_id = parts[0].strip()\n",
    "            book_id = parts[4].strip()\n",
    "            chapter_title = parts[6].strip()\n",
    "            try: duration = float(parts[2].strip())\n",
    "            except: duration = None\n",
    "            chapters_info[chapter_id] = {\n",
    "                \"book_id\": book_id,\n",
    "                \"chapter_title\": chapter_title,\n",
    "                \"duration\": duration\n",
    "            }\n",
    "    return chapters_info\n",
    "\n",
    "\n",
    "def parse_textgrid_segments_with_blanks(tg_path: Path):\n",
    "    \"\"\"\n",
    "    Define segments including whitespace (\"\") intervals:\n",
    "    - seg0: start=0.0, end=the first whitespace xmax (if none, use the last word xmax of the first chunk)\n",
    "    - regular segments: start=previous whitespace xmin, end=next whitespace xmax\n",
    "    - if there is no \"next whitespace\" for the final segment, set end=last word xmax\n",
    "    \"\"\"\n",
    "\n",
    "    tg = TextGrid.fromFile(str(tg_path))\n",
    "\n",
    "    word_tier = next((t for t in tg.tiers if t.name and t.name.lower() in [\"words\", \"word\"]), None)\n",
    "    phone_tier = next((t for t in tg.tiers if t.name and t.name.lower() in [\"phones\", \"phonemes\"]), None)\n",
    "    if not word_tier:\n",
    "        return []\n",
    "\n",
    "    intervals = [(Decimal(str(iv.minTime)), Decimal(str(iv.maxTime)), (iv.mark or \"\").strip())\n",
    "                 for iv in word_tier.intervals]\n",
    "\n",
    "    segments = []\n",
    "    prev_blank = None\n",
    "    i, L = 0, len(intervals)\n",
    "\n",
    "    while i < L:\n",
    "        xmin, xmax, mark = intervals[i]\n",
    "\n",
    "        # Whitespace intervals are recorded as prev_blank and skipped\n",
    "        if mark == \"\":\n",
    "            prev_blank = (xmin, xmax)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Collect word chunk\n",
    "        chunk_words = []\n",
    "        last_word_xmax = xmax\n",
    "        while i < L:\n",
    "            xxmin, xxmax, m = intervals[i]\n",
    "            if m == \"\":\n",
    "                break\n",
    "            chunk_words.append((m, xxmin, xxmax))\n",
    "            last_word_xmax = xxmax\n",
    "            i += 1\n",
    "\n",
    "        # Next whitespace\n",
    "        next_blank = intervals[i] if (i < L and intervals[i][2] == \"\") else None\n",
    "\n",
    "        # Calculate boundaries\n",
    "        if prev_blank is None:\n",
    "            seg_start = Decimal(\"0.0\")  # First segment of the file\n",
    "        else:\n",
    "            seg_start = prev_blank[0]   # Previous whitespace xmin\n",
    "\n",
    "        if next_blank is not None:\n",
    "            seg_end = next_blank[1]     # Next whitespace xmax\n",
    "        else:\n",
    "            seg_end = last_word_xmax    # If no trailing whitespace, use last word xmax\n",
    "\n",
    "        if chunk_words:\n",
    "            # Count syllables (phones)\n",
    "            num_syllables = 0\n",
    "            if phone_tier:\n",
    "                for p in phone_tier.intervals:\n",
    "                    pmin = Decimal(str(p.minTime))\n",
    "                    pmax = Decimal(str(p.maxTime))\n",
    "                    if pmax <= seg_start or pmin >= seg_end:\n",
    "                        continue\n",
    "                    if VOWEL_PATTERN.match((p.mark or \"\").strip()):\n",
    "                        num_syllables += 1\n",
    "\n",
    "            segments.append({\n",
    "                \"words\": chunk_words,      # Absolute coordinates\n",
    "                \"start\": seg_start,\n",
    "                \"end\": seg_end,\n",
    "                \"duration\": (seg_end - seg_start),\n",
    "                \"num_syllables\": num_syllables\n",
    "            })\n",
    "\n",
    "        # If i is whitespace, update prev_blank for the next loop\n",
    "        if i < L and intervals[i][2] == \"\":\n",
    "            prev_blank = (intervals[i][0], intervals[i][1])\n",
    "            i += 1\n",
    "\n",
    "    return segments\n",
    "\n",
    "# =========================\n",
    "# Frame-level label generation\n",
    "# =========================\n",
    "def _times_to_frame_indices(times_sec: np.ndarray, sr: int, hop: int, total_frames: int):\n",
    "    if times_sec.size == 0:\n",
    "        return np.empty((0,), dtype=np.int32)\n",
    "    frames = np.floor(times_sec * (sr / hop)).astype(np.int32)\n",
    "    return frames[(frames >= 0) & (frames < total_frames)]\n",
    "\n",
    "def make_frame_label_for_segment(seg_start: float, seg_end: float, word_abs_starts: np.ndarray):\n",
    "    \"\"\"\n",
    "    Given a segment absolute time window [seg_start, seg_end] and an array of\n",
    "    absolute word start times (word_abs_starts):\n",
    "    - relative start time = word_abs_start - seg_start\n",
    "    - total_frames = floor(duration * sr / hop) + 1\n",
    "    - return a vector (list[int]) with 1s only at onset frames\n",
    "    \"\"\"\n",
    "    duration = float(seg_end - seg_start)\n",
    "    total_frames = int(np.floor(duration * SR / HOP)) + 1\n",
    "    if total_frames <= 0:\n",
    "        return [], total_frames\n",
    "\n",
    "    rel_starts = word_abs_starts - float(seg_start)\n",
    "    frame_indices = _times_to_frame_indices(rel_starts, SR, HOP, total_frames)\n",
    "\n",
    "    label = np.zeros((total_frames,), dtype=np.int8)\n",
    "    if frame_indices.size > 0:\n",
    "        label[frame_indices] = 1\n",
    "    return label.tolist(), total_frames\n",
    "\n",
    "def process_textgrid_file(tg_path: Path,\n",
    "                          audio_base_dir: Path,\n",
    "                          speakers_info: dict,\n",
    "                          chapters_info: dict,\n",
    "                          subset: str,\n",
    "                          speaker_id: str,\n",
    "                          chapter_id: str):\n",
    "    \"\"\"\n",
    "    Process a single TextGrid file and return metadata rows for each segment.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    try:\n",
    "        segments = parse_textgrid_segments_with_blanks(tg_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {tg_path}: {e}\")\n",
    "        return []\n",
    "        \n",
    "    if not segments:\n",
    "        return rows\n",
    "\n",
    "    utt_id_prefix = tg_path.stem\n",
    "    audio_path = audio_base_dir / speaker_id / chapter_id / f\"{utt_id_prefix}.flac\"\n",
    "\n",
    "    for i, seg in enumerate(segments):\n",
    "        # Text / statistics\n",
    "        full_text = \" \".join([w[0] for w in seg[\"words\"]])\n",
    "        num_words, num_chars = count_words_and_chars(full_text)\n",
    "        num_syllables = int(seg[\"num_syllables\"])\n",
    "\n",
    "        # Absolute word start/end\n",
    "        word_abs_starts = np.array([float(w[1]) for w in seg[\"words\"]], dtype=np.float64)\n",
    "        \n",
    "        # Generate frame_label (immediately at meta stage)\n",
    "        frame_label_list, total_frames = make_frame_label_for_segment(seg[\"start\"], seg[\"end\"], word_abs_starts)\n",
    "\n",
    "        # words_info: absolute time (or relative time if needed later)\n",
    "        words_info = [\n",
    "            {\"text\": w[0], \"start\": float(w[1]), \"end\": float(w[2])} for w in seg[\"words\"]\n",
    "        ]\n",
    "\n",
    "        rows.append({\n",
    "            \"utt_id\": f\"{utt_id_prefix}_seg{i}\",\n",
    "            \"subset\": subset,\n",
    "            \"speaker_id\": speaker_id,\n",
    "            \"chapter_id\": chapter_id,\n",
    "            \"transcript\": full_text,\n",
    "            \"num_words\": num_words,\n",
    "            \"num_chars\": num_chars,\n",
    "            \"num_syllables\": num_syllables,\n",
    "            \"start_time\": float(seg[\"start\"]),\n",
    "            \"end_time\": float(seg[\"end\"]),\n",
    "            \"duration\": float(seg[\"duration\"]),\n",
    "            \"audio_path\": str(audio_path),\n",
    "            \"relative_audio_path\": str(audio_path).split(\"LibriSpeech/LibriSpeech/\")[-1] if \"LibriSpeech/LibriSpeech/\" in str(audio_path) else str(audio_path),\n",
    "            \"gender\": speakers_info.get(speaker_id, {}).get(\"gender\", \"NA\"),\n",
    "            \"book_id\": chapters_info.get(chapter_id, {}).get(\"book_id\", \"NA\"),\n",
    "            \"chapter_title\": chapters_info.get(chapter_id, {}).get(\"chapter_title\", \"NA\"),\n",
    "            \"chapter_duration\": chapters_info.get(chapter_id, {}).get(\"duration\", \"NA\"),\n",
    "            # frame-level\n",
    "            \"frame_label_list\": frame_label_list,                  # python list (원본)\n",
    "            \"frame_label\": \",\".join(map(str, frame_label_list)),   # CSV 저장 친화\n",
    "            \"n_frames\": total_frames,\n",
    "            \"words_info\": words_info                               # JSON 직렬화 가능\n",
    "        })\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Reader Directory Processing\n",
    "# =========================\n",
    "def process_reader_dir(reader_dir: Path,\n",
    "                       alignment_base_dir: Path,\n",
    "                       audio_base_dir: Path,\n",
    "                       speakers_info: dict,\n",
    "                       chapters_info: dict):\n",
    "    \"\"\"\n",
    "    reader_dir = alignments/<subset>/<speaker>\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    subset = reader_dir.parent.name\n",
    "    speaker_id = reader_dir.name\n",
    "\n",
    "    for chapter_dir in sorted([d for d in reader_dir.glob(\"*\") if d.is_dir()]):\n",
    "        chapter_id = chapter_dir.name\n",
    "        for tg_path in sorted(chapter_dir.glob(\"*.TextGrid\")):\n",
    "            rows.extend(\n",
    "                process_textgrid_file(\n",
    "                    tg_path=tg_path,\n",
    "                    audio_base_dir=audio_base_dir / subset,\n",
    "                    speakers_info=speakers_info,\n",
    "                    chapters_info=chapters_info,\n",
    "                    subset=subset,\n",
    "                    speaker_id=speaker_id,\n",
    "                    chapter_id=chapter_id\n",
    "                )\n",
    "            )\n",
    "    return rows    \n",
    "\n",
    "def load_librispeech_counts_metadata(data_dir, align_dir, subset_filters=None):\n",
    "    \"\"\"\n",
    "    WRAPPER: Uses new logic to load metadata\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    align_path = Path(align_dir)\n",
    "    \n",
    "    # Try to find metadata files\n",
    "    speakers_map = {}\n",
    "    chapters_map = {}\n",
    "    \n",
    "    candidates = [data_path, data_path / \"LibriSpeech\", data_path.parent]\n",
    "    \n",
    "    spk_file = next((p / \"SPEAKERS.TXT\" for p in candidates if (p / \"SPEAKERS.TXT\").exists()), None)\n",
    "    ch_file = next((p / \"CHAPTERS.TXT\" for p in candidates if (p / \"CHAPTERS.TXT\").exists()), None)\n",
    "    \n",
    "    if spk_file: \n",
    "        print(f\"Loading speakers info from {spk_file}\")\n",
    "        speakers_map = parse_speakers_txt(spk_file)\n",
    "    if ch_file: \n",
    "        print(f\"Loading chapters info from {ch_file}\")\n",
    "        chapters_map = parse_chapters_txt(ch_file)\n",
    "    \n",
    "    all_rows = []\n",
    "    \n",
    "    # align_dir is expected to be e.g. .../train-clean-100\n",
    "    if not align_path.exists():\n",
    "        print(f\"Align dir {align_path} not found.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    children = [d for d in align_path.iterdir() if d.is_dir() and d.name.isdigit()]\n",
    "    print(f\"Found {len(children)} speaker folders in {align_path}\")\n",
    "    \n",
    "    for spk_dir in tqdm(children, desc=\"Processing Speakers\"):\n",
    "        audio_root = data_path\n",
    "        \n",
    "        rows = process_reader_dir(spk_dir, align_path.parent, audio_root, speakers_map, chapters_map)\n",
    "        all_rows.extend(rows)\n",
    "            \n",
    "    return pd.DataFrame(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584edb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiprocessing Workers\n",
    "\n",
    "_augmentor = None\n",
    "_extractor = None\n",
    "\n",
    "def _init_worker(aug_args, feature_args):\n",
    "    global _augmentor, _extractor\n",
    "    _augmentor = Augmentor(**aug_args)\n",
    "    _extractor = Extractor(**feature_args)\n",
    "\n",
    "def _process_item(row):\n",
    "    \"\"\"\n",
    "    Process a single metadata row: Load Audio -> Augment -> Extract Feature\n",
    "    \"\"\"\n",
    "    audio_path = row['audio_path']\n",
    "    utt_id = row['utt_id']\n",
    "    start_time = row.get('start_time', 0.0)\n",
    "    end_time = row.get('end_time', None)\n",
    "    \n",
    "    try:\n",
    "        # Load Audio (Full File)\n",
    "        wav, sr = torchaudio.load(audio_path)\n",
    "        if sr != 16000:\n",
    "            wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "        wav = wav.mean(dim=0) # Mix to mono if stereo\n",
    "        \n",
    "        # Crop segment if start/end time provided (from TextGrid segmentation)\n",
    "        # Note: The new metadata logic provides segments. \n",
    "        if end_time is not None:\n",
    "             s_sample = int(start_time * 16000)\n",
    "             e_sample = int(end_time * 16000)\n",
    "             if e_sample > wav.size(0): e_sample = wav.size(0)\n",
    "             if s_sample < wav.size(0):\n",
    "                 wav = wav[s_sample:e_sample]\n",
    "                 \n",
    "        # Augment & Extract\n",
    "        if _augmentor: wav = _augmentor(wav)\n",
    "        feat = _extractor(wav) # (T, n_mels)\n",
    "        \n",
    "        # --- Frame Labels (Onset) ---\n",
    "        T = feat.shape[0]\n",
    "        \n",
    "        # Use pre-calculated frame_label_list from metadata\n",
    "        frame_label_list = row.get('frame_label_list', [])\n",
    "        \n",
    "        # Handle string serialization from CSV\n",
    "        if isinstance(frame_label_list, str):\n",
    "            try:\n",
    "                frame_label_list = ast.literal_eval(frame_label_list)\n",
    "            except:\n",
    "                frame_label_list = []\n",
    "                \n",
    "        # If list is empty or missing, fallback or zeros\n",
    "        if not frame_label_list:\n",
    "             frame_labels = np.zeros(T, dtype=np.float32)\n",
    "        else:\n",
    "             # Truncate or pad to match feature length T\n",
    "             # Metadata total_frames might differ slightly due to rounding vs stft padding\n",
    "             fl = np.array(frame_label_list, dtype=np.float32)\n",
    "             if len(fl) > T:\n",
    "                 frame_labels = fl[:T]\n",
    "             elif len(fl) < T:\n",
    "                 frame_labels = np.pad(fl, (0, T - len(fl)))\n",
    "             else:\n",
    "                 frame_labels = fl\n",
    "        \n",
    "        return {\n",
    "            'utt_id': utt_id,\n",
    "            'feat': feat.cpu().numpy(), # Save as numpy to save space/pickle\n",
    "            'frame_label': frame_labels,\n",
    "            'word_count': row['num_words'],\n",
    "            'syllable_count': row['num_syllables'],\n",
    "            'duration': row['duration'],\n",
    "            'audio_path': audio_path\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # print(f\"Failed {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_dataset_parallel(meta_df, save_path, aug_args=None, feature_args=None, num_workers=None):\n",
    "    if num_workers is None: num_workers = max(1, cpu_count() - 2)\n",
    "    \n",
    "    if aug_args is None: aug_args = {}\n",
    "    if feature_args is None: feature_args = {}\n",
    "    \n",
    "    # Convert DataFrame to list of dicts for pool\n",
    "    data_list = meta_df.to_dict('records')\n",
    "    results = []\n",
    "\n",
    "    print(f\"Processing {len(data_list)} files with {num_workers} workers...\")\n",
    "    \n",
    "    with Pool(processes=num_workers, initializer=_init_worker, initargs=(aug_args, feature_args)) as pool:\n",
    "        for res in tqdm(pool.imap_unordered(_process_item, data_list), total=len(data_list)):\n",
    "            if res is not None:\n",
    "                results.append(res)\n",
    "    \n",
    "    # Save Results\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        print(f\"Saving {len(results)} items to {save_path}...\")\n",
    "        torch.save(results, save_path) # Save as list of dicts\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21567a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MAIN EXECUTION ---\n",
    "\n",
    "# Define Paths\n",
    "libri_data_dir = ...\n",
    "libri_algn_dir = ...\n",
    "libri_processed_dir = ...\n",
    "\n",
    "# Define Arguments\n",
    "feature_args = {'n_mels': 24, 'apply_cmvn': True}\n",
    "\n",
    "# Paths for augmentation resources\n",
    "aug_args = {\n",
    "    'rir_paths': ..., # UPDATE THIS\n",
    "    'noise_paths': ..., # UPDATE THIS\n",
    "    'rir_prob': 0.5, \n",
    "    'noise_prob': 0.5\n",
    "}\n",
    "\n",
    "noaug_args = {'rir_prob': 0, 'noise_prob': 0}\n",
    "splits = [\"test-clean\", \"dev-clean\", \"test-other\", \"dev-other\", \"train-clean-100\", \"train-clean-360\", \"train-other-500\"]\n",
    "\n",
    "grouped_files = {\n",
    "    'train': [],\n",
    "    'valid': [],\n",
    "    'test': []\n",
    "}\n",
    "\n",
    "processed_dir = os.path.join(libri_processed_dir, \"processed\")\n",
    "meta_dir = os.path.join(libri_processed_dir, \"meta\")\n",
    "\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "os.makedirs(meta_dir, exist_ok=True)\n",
    "\n",
    "for split in splits:\n",
    "    print(f\"\\n=== Processing {split} ===\")\n",
    "        \n",
    "    meta_path = os.path.join(meta_dir, f\"librispeech_{split.replace('_','-')}_utt_meta_withframe.csv\")\n",
    "    feat_path = os.path.join(processed_dir, f\"librispeech_{split.replace('_','-')}_logmel_features_withframe.pt\")\n",
    "\n",
    "    # Categorize splits for later merging\n",
    "    if \"train\" in split:\n",
    "        grouped_files['train'].append(feat_path)\n",
    "    elif \"dev\" in split:\n",
    "        grouped_files['valid'].append(feat_path)\n",
    "    elif \"test\" in split:\n",
    "        grouped_files['test'].append(feat_path)\n",
    "\n",
    "    # 1. Generate Metadata\n",
    "    meta_df = None\n",
    "    if os.path.exists(meta_path):\n",
    "        print(f\"Loading existing metadata from {meta_path}...\")\n",
    "        meta_df = pd.read_csv(meta_path)\n",
    "        # Check if 'frame_label_list' column exists (needed for frame labels)\n",
    "        if 'frame_label_list' not in meta_df.columns:\n",
    "            print(\"Metadata missing 'frame_label_list'. Regenerating...\")\n",
    "            meta_df = None\n",
    "\n",
    "    if meta_df is None:\n",
    "        print(\"Generating metadata...\")\n",
    "        subset_align_dir = os.path.join(libri_algn_dir, split.replace('_','-'))\n",
    "        if not os.path.exists(subset_align_dir):\n",
    "            subset_align_dir = libri_algn_dir \n",
    "            \n",
    "        meta_df = load_librispeech_counts_metadata(libri_data_dir, subset_align_dir, subset_filters=[split.replace('_','-')])\n",
    "        # Save frame_label_list as literal string for CSV\n",
    "        meta_df.to_csv(meta_path, index=False)\n",
    "        print(f\"Saved metadata to {meta_path}\")\n",
    "\n",
    "    # 2. Generate Features\n",
    "    is_train = \"train\" in split\n",
    "    current_aug = aug_args if is_train else noaug_args\n",
    "    \n",
    "    # Check if feature file already exists\n",
    "    if os.path.exists(feat_path):\n",
    "        print(f\"Features already exist: {feat_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Process Parallel & Collect Shards\n",
    "    print(f\"Starting parallel processing for features...\")\n",
    "    shards_dir = os.path.join(processed_dir, f\"{split}_shards\")\n",
    "    os.makedirs(shards_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate & Save Shards (Chunked Processing)\n",
    "    dataset_results = generate_dataset_parallel(\n",
    "        meta_df, \n",
    "        None, \n",
    "        aug_args=current_aug, \n",
    "        feature_args=feature_args\n",
    "    )\n",
    "    \n",
    "    # 3. Merge Shards/Results into Final Dictionary\n",
    "    print(\"Merging results...\")\n",
    "    final_data = {}\n",
    "    for res in dataset_results:\n",
    "        utt_id = res['utt_id']\n",
    "        final_data[utt_id] = {\n",
    "            \"feat\": torch.tensor(res['feat']), \n",
    "            \"frame_label\": torch.tensor(res['frame_label']), # Include frame_label\n",
    "            \"word_count\": res['word_count'],\n",
    "            \"syllable_count\": res['syllable_count'],\n",
    "            \"duration\": res['duration'],\n",
    "            \"audio_path\": res.get('audio_path', '')\n",
    "        }\n",
    "        \n",
    "    print(f\"Saving merged file to {feat_path}...\")\n",
    "    torch.save(final_data, feat_path)\n",
    "    print(\"Saved.\")\n",
    "    \n",
    "# --- Aggregate Category Merging ---\n",
    "print(\"\\n=== Merging Categories (Total Train, Valid, Test) ===\")\n",
    "\n",
    "for category, file_paths in grouped_files.items():\n",
    "    print(f\"Merging {category} set from {len(file_paths)} files...\")\n",
    "    if not file_paths:\n",
    "        print(f\"No files found for {category}, skipping.\")\n",
    "        continue\n",
    "        \n",
    "    merged_data = {}\n",
    "    for fp in file_paths:\n",
    "        if os.path.exists(fp):\n",
    "            print(f\"  Loading {fp}...\")\n",
    "            data_part = torch.load(fp)\n",
    "            merged_data.update(data_part)\n",
    "        else:\n",
    "            print(f\"  Warning: File {fp} not found, skipping.\")\n",
    "    \n",
    "    save_name = f\"total_{category}.pt\"\n",
    "    save_path = os.path.join(processed_dir, save_name)\n",
    "    print(f\"Saving combined {category} set ({len(merged_data)} samples) to {save_path}...\")\n",
    "    torch.save(merged_data, save_path)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
